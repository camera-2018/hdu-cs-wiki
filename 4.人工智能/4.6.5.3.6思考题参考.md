# 思考题参考

思考并无绝对的对错，此处仅供参考，希望大家能在自己的思考的基础上再来这里解决思考的疑惑。

## 

## 思考 1

### 1.1

[CNN 与 MLP 之间的关系，优缺点](https://www.editcode.net/archive/detail/89781)

[MLP(多层感知机)只是 CNN(卷积网络)的一个特例](https://blog.csdn.net/u010165147/article/details/82851717#:~:text=%E6%98%BE%E7%84%B6%E5%8F%AF%E4%BB%A5%E6%8E%A8%E5%AF%BC%E5%87%BA%EF%BC%8C%E5%BD%93%20CNN%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A4%A7%E5%B0%8F%E4%B8%8E%E8%BE%93%E5%85%A5%E5%A4%A7%E5%B0%8F%E7%9B%B8%E5%90%8C%20%E6%97%B6%E5%85%B6%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E7%AD%89%E4%BB%B7%E4%BA%8EMLP%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4MLP%E7%AD%89%E4%BB%B7%E4%BA%8E,%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A4%A7%E5%B0%8F%E4%B8%8E%E6%AF%8F%E5%B1%82%E8%BE%93%E5%85%A5%E5%A4%A7%E5%B0%8F%E7%9B%B8%E5%90%8C%20%E7%9A%84CNN%EF%BC%88%E5%A6%82%E8%BE%93%E5%85%A5%E5%9B%BE%E7%89%87%E4%B8%BA100x100%EF%BC%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A4%A7%E5%B0%8F%E4%B8%BA100x100%EF%BC%89%EF%BC%8C%E6%89%80%E4%BB%A5MLP%E6%98%AFCNN%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E4%BE%8B%E3%80%82%20%E8%80%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A4%A7%E5%B0%8F%E4%B8%8E%E6%AF%8F%E5%B1%82%E8%BE%93%E5%85%A5%E5%A4%A7%E5%B0%8F%E7%9B%B8%E5%90%8C%E4%BC%9A%E7%9B%B4%E6%8E%A5%E4%B8%A2%E5%A4%B1%E9%9D%9E%E5%B8%B8%E5%A4%9A%E7%9A%84%E8%BE%93%E5%85%A5%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%EF%BC%8C%E6%89%80%E4%BB%A5MLP%E8%BF%99%E7%A7%8D%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%E4%B8%8D%E9%80%82%E5%90%88%E5%9B%BE%E5%83%8F%E8%BF%99%E7%A7%8D%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E4%B8%B0%E5%AF%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82)

### 1.2

[深度理解感受野](https://blog.csdn.net/weixin_40756000/article/details/117264194)

### 1.3

[卷积神经网络中的平移不变性](https://zhuanlan.zhihu.com/p/382926269)

### 1.5

[你真的看懂 Relu 了吗？大家都说是非线性，为什么我怎么看都是线性啊？](https://zhuanlan.zhihu.com/p/405068757)

### 1.6

[什么是深度学习中的卷积?](https://zhuanlan.zhihu.com/p/140550547)

## 思考 2

### 2.1

[深度学习端到端的理解](https://blog.csdn.net/Bulldozer_GD/article/details/95071826)

### 2.2

[反卷积详解](https://blog.csdn.net/bestrivern/article/details/89553513)

### 2.3

### 2.4

[语义分割概念及应用介绍](https://zhuanlan.zhihu.com/p/46200875)

## 思考 3

### 3.1

[Batch Normalization（BN 层）详解](https://www.jianshu.com/p/b05282e9ca57)

### 3.2

[ResNet 残差、退化等细节解读](https://blog.csdn.net/a8039974/article/details/122380735)

## 思考 4

### 4.1

[U-Net 和 ResNet：长短跳跃连接的重要性（生物医学图像分割）](https://www.jianshu.com/p/20a47427823c)
