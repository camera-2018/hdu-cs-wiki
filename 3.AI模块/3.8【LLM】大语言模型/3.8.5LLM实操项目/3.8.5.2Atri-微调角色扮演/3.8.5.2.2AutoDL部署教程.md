# AutoDL亚托莉训练
- 先租一台pytorch>=2.0且显存>=24GB的机子
主要参照以下两个链接：
1. https://github.com/datawhalechina/self-llm/tree/master
2. https://github.com/baichuan-inc/Baichuan2

## 第一步: 安装环境依赖
```python
source /etc/network_turbo //(设置学术加速)/可以通过其他方式实现

python -m pip install --upgrade pip
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.9.5
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
pip install transformers==4.33.1
pip install peft==0.4.0
pip install datasets==2.10.1
pip install accelerate==0.20.3
pip install tiktoken
pip install transformers_stream_generator
```

## 第二步: 安装模型(本例选用百川7B)
```
cd autodl-tmp
touch download.py
vim download.py
"i"
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('baichuan-inc/Baichuan2-7B-Chat',cache_dir='/root/autodl-tmp', revision='v1.0.4')
"esc"
":wq"
python download.py
```

## 第三步: 克隆项目仓库
```
source /etc/network_turbo //(设置学术加速)/可以通过其他方式实现
git clone https://github.com/datawhalechina/self-llm
git clone https://github.com/baichuan-inc/Baichuan2
```

## 第四步: 微调训练
- 将训练文件(ATRI.json)放入dataset文件夹;
- 修改/self-llm/BaiChuan/04-Baichuan2-7B-chat Lora 微调.ipynb中的所有模型和文件路径
- 在模型目录底下新建文件夹"output",修改代码路径,用于保存输出:
```
/root/autodl-tmp/baichuan-inc/Baichuan2-7B-Chat/output
```
- 选择本地的python环境下的conda,逐步的运行代码(适当修改提示词)
- 将/root/autodl-tmp/Baichuan2/web_demo.py中的init_model函数改成如下格式(记得保存):
```python
from transformers import AutoModelForSeq2SeqLM
from peft import PeftModel, PeftConfig

def init_model():
    peft_model_id = "/root/autodl-tmp/baichuan-inc/Baichuan2-7B-Chat/output/checkpoint-300"             
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config = PeftConfig.from_pretrained(peft_model_id)
    model = AutoModelForCausalLM.from_pretrained(
        "/root/autodl-tmp/baichuan-inc/Baichuan2-7B-Chat",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model = model.to(device)
    model.generation_config = GenerationConfig.from_pretrained(
        "/root/autodl-tmp/baichuan-inc/Baichuan2-7B-Chat"
    )
    model = PeftModel.from_pretrained(model, peft_model_id)
    tokenizer = AutoTokenizer.from_pretrained(
        "/root/autodl-tmp/baichuan-inc/Baichuan2-7B-Chat",
        use_fast=False,
        trust_remote_code=True
    )
    model.eval()
    return model, tokenizer
```

## 第五步: 启动
- 我们要启动conda环境,否则cpu跑起来效率低下:
```
conda init bash(只有第一次要)
conda activate /root/miniconda3
```

- 重启你的ssh链接,再清除你finetune后的python内核(在ipynb的页面清除输出并重启内核) (本例使用ssh链接)
```
conda activate /root/miniconda3
```
- 输入以下代码启动,点击弹出的链接,等待3-5分钟:(可以提前创建端口)
```
streamlit run web_demo.py --server.address 127.0.0.1 --server.port 6006
```
- 输入第一句话,提示模型开始角色扮演

笔者的参数:
- Temperature 0.4
- r=6
- gradient_accumulation_steps=1
- num_train_epochs=5
- 选择checkpoint-700
- 首句对话应该类似如下:现在你要扮演二次元美少女游戏的女子主角--亚托莉，我扮演夏生
