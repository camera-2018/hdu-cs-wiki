# Atri-微调角色扮演
> Author:Sky | Editor:S4kana
## 一、引言
亚托莉是Galgame中的机器人少女，是一个拥有自我感情的机器人，其欢脱的性格受到很多人的喜爱

因此笔者团队从扒数据开始，在完全零基础的情况下经过两个月的时间，参考多个教程实现了测试版本的亚托莉（虽然经常会出戏）

![图片](https://cdn.xyxsw.site/1djklfhjdlsf.jpg)

> 图为stable diffusion通过lora微调后生成的

![图片](https://cdn.xyxsw.site/2fgsdfsadsaf.jpg)

## 二、大语言模型（LLM）
### 1-LLM背景
大语言模型（LLMs）的发展背景始于自然语言处理（NLP）的需求。

这一领域旨在使计算机能够理解和生成人类语言。随着数据量的增加和计算能力的提升，这些模型变得更加强大和复杂，特别是在过去的十年中，由于深度学习技术的进步，语言模型的性能得到了显著的提高。相关产品也孕育而生。

::: tip **🤔NLP是什么？和LLMs是什么关系？**
  自然语言处理（NLP）是一门涉及计算机科学、人工智能和语言学等领域的学科，旨在使计算机能够理解、解释和处理人类语言。NLP的目标包括：
  - 语言理解（即让计算机能够理解人类语言的含义）
  - 语言生成（即让计算机能够生成符合语法和语义规则的语言）、
  - 机器翻译、问答系统、文本分类、情感分析……
  
  大语言模型（LLMs）是NLP领域中的一种重要技术，用于处理自然语言。LLMs通过大规模的文本数据进行训练，学习语言的统计规律，从而能够对文本进行生成、理解和处理。
 
  因此，LLMs是NLP领域中的一个重要分支，它们为实现NLP任务提供了强大的工具和基础。
  
  我们对于NLP也有很多的研究项目在进行！感兴趣可以来了解一下。
:::

### 2-LLM原理
大语言模型（LLMs）的原理基于“神经网络”，这是一种模仿人脑工作方式的算法结构。

这些模型利用大量的文本数据进行训练，通过学习语言的统计规律来理解和生成文本。在训练过程中，模型试图预测文本中的下一个单词，这样的预测任务使模型学习到了语言中复杂的结构和单词之间的关联。

这种基于神经网络的方法使得大语言模型能够在各种自然语言处理任务中表现出色，包括语言生成、文本理解、情感分析等。

### 3-LLM架构（关键概念）
**1. Transformer架构**

大多数现代大语言模型采用的是Transformer架构。

它主要基于“自注意力”机制，允许模型在处理输入的单词时考虑到其他单词的影响，这种机制帮助模型更好地理解文本中的上下文。Transformer架构主要分为以下几部分：
  - 输出嵌入层：将输入文本化成向量
  - 位置编码：由于transformer不具备处理序列的能力，所以要引入位置编码，可以帮助模型理解词语在句子中的关系，也是一个向量
  - 多头注意力机制：核心部分，允许模型在处理一个词的同时，获取其他的与这个词之间的关系
  - 前馈神经网络：多头注意力后，前馈网络会对每个位置的输出独立的处理
  - 层归一化与残差连接：避免训练中出现梯度消失
  - 输出层：将处理完的数据转化成为所需结果

![图片](https://cdn.xyxsw.site/853f625e-e21a-47c1-96da-e085e4d74cd9.jpg)

::: tip **🤔说了这么多，Transformer架构在LLMs中起到什么作用呢？**
简单来说，Transformer作为LLM的核心，承担起理解文字含义的重大责任！

Transformer的由于其核心——自注意力机制，使得模型能够在输入序列中捕捉长距离的依赖关系，这对于理解和生成自然语言文本至关重要。

在大语言模型中，Transformer被用作编码器和解码器的基础组件，通过多层堆叠的Transformer结构来处理输入文本并生成相应的输出。
:::

**2. 层和参数**

一个典型的大语言模型包括多个层次，每个层次都包括数百万到数十亿的参数。

这些参数在训练过程中不断调整，以更准确地反映语言的使用和语义。更多的层和参数通常意味着更强的学习能力和更复杂的表达能力。

::: tip **🤔层和参数是如何影响LLMs的学习能力和表达能力的？**
  - 学习能力
    每个参数都代表了模型中的可调整项，它们在训练过程中不断地根据数据进行更新和调整，以使模型能够更准确地拟合训练数据。更多的参数意味着模型能够学习到更复杂和更细微的模式和规律，从而提高了模型的学习能力和泛化能力。
  - 表示能力
    每一层都包含了一系列的神经网络单元，这些单元通过参数来对输入数据进行变换和映射。更多的层次意味着模型能够在更多的抽象层次上对输入进行理解和表达，从而提高了模型的表示能力。
在大语言模型中，通过增加层次和参数数量，可以提高模型对语言的理解和生成能力。

然而，需要注意的是，增加层次和参数数量也可能增加模型的计算复杂度和训练成本，因此需要在表示能力和计算效率之间进行权衡。
:::

**3. 微调**

尽管这些模型在训练后已经相当强大，但它们通常还需要通过微调来适应特定的任务或领域。

微调是在模型训练完成后，使用特定任务的数据再进行一次较短的训练，这使得模型能够调整其参数以更好地适应特定的应用场景。

接下来会详细介绍微调Atri模型的整个过程。

## 三、训练过程：模型微调（Finetune）
**1. 微调的定义与重要性**

微调是深度学习中一种常用的技术，它主要涉及将一个大规模数据集上预训练的模型调整以适应特定的、通常规模较小的数据集。

对于大多数人而言，从头开始构建一个大规模模型几乎是不可能的，因为这需要大量的数据收集和高昂的时间成本及金钱成本。

通过在已有的模型上进行微调，不仅可以提升模型在特定场景下的表现，还能显著降低成本。

**2. 微调的实施步骤**
1. 模型选择
  
近年来，各种深度学习模型层出不穷。例如，对于生成式的语言模型，如ChatGPT-4，这类模型可以满足基本的对话和聊天需求。

在众多模型中，我们选择了“Baichuan”，因其在角色扮演领域表现突出。

2. 数据集准备

选择适当的数据集对于微调的成功至关重要。

我们选择了基于“亚托莉”这一作品的数据集，尽管其间断式对话的特性使得数据集质量一般，但这一选择依然具有其独到之处。此类数据的不稳定性可能导致训练后的效果存在波动。

3. 微调方法选择

我们采用了“LoRA”（低秩调整）方法，通过在模型的自注意力和前馈网络中添加低秩矩阵来进行调整。

这种方法在更新模型时只需修改少量参数，因此训练效率较高，易于实施，并且在数据较少的情况下不容易过拟合。

4. 训练平台选择

为提高效率，我们选择在网络云服务器平台上租用显卡进行模型的训练工作。

![图片](https://cdn.xyxsw.site/9b9d72ef-65be-4f41-a8fe-cc3c7fd054eb.jpg)

## 三、模型部署
- 详细部署方法见下文

## 四、后续展望
目前的效果并没有理想中那么好，因此我们期望可以在后续继续调整效果，也欢迎其他同学可以在我们基础上改进并调优。

**可选择方向：**
1. 进一步调整参数
2. 使用规模更大的模型
3. 将维基百科的背景故事内容做成对话加到数据集里
4. 对数据集进行清洗，将重复出现的如"嗯·······"，反复出现多次，可增加描述

未完待续……

## 五、翻车记录
（骂人ATRI）
![图片](https://cdn.xyxsw.site/4e9fc406-6bf0-4deb-a543-36b2c43c63dc.jpg)

（人形兵器）
![图片](https://cdn.xyxsw.site/f05a0134-e2f6-4c01-a0e5-fd2494423d79.jpg)

![图片](https://cdn.xyxsw.site/025c4a63-c498-4089-a9c0-1b98b70dc221.jpg)

（亚托利桑，日本的樱花开了，你也想家了吗开始说家乡话了）
![图片](https://cdn.xyxsw.site/dbee07cd-d75e-4afc-b807-6198e8292a62.jpg)

（显卡说话了）
![图片](https://cdn.xyxsw.site/536e7b24-b9ad-42d5-82b7-38f63ed39371.jpg)

## 六、附录：对数据集的操作
**数据集制作：**

常常选择json格式作为训练文件,主要原因如下:
- 数据对人与机器都有较强的可读性
- 严格结构化的存储内容
- 兼容性强,适用于各种模型与模式
- 采用字典与列表的内部结构,易于表达复杂内容
- 易于修改与处理,切片方便

**本次亚托莉数据集的制作流程：**
1. 搜集原作全文本txt;
2. 利用正则化表达式提取对话元素;
3. 筛选男女主的二人对话;
4. 去除噪声(干扰项);
5. 规范格式
