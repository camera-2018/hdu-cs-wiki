::: warning 施工中🚧
格式内容还没修改，部分内容还未完工，仅为粗糙版本，敬请期待。

关于本模块的旧版内容，你可以点击[这里](/2023旧版内容/旧版内容索引)查看。
:::

# LLM基础

## LLM做了什么？/LLM的工作原理

在我们与chatgpt进行对话的过程中，只需要输入你的问题或者请求，LLM会根据其所学习的数据和模式来生成回应。

### 输入->模型->输出

我们可以把模型理解为一个函数，接收输入，处理后输出。

这个函数中有着大量的参数。

### LLM其实是在做文字接龙

*（文字接龙是一种可以不断重复执行的操作。）*

> 例如，如果您在 LLM中输入：“How are”，它将计算下一个词的可能性。
>

>
> 它可以将 60% 的概率分配给“you”，20% 的概率“things”，依此类推。

> 它会选择概率最大的字。接着，它会将这个字加至原来输入的末尾，即“How are you”，作为新的输入继续文字接龙。
>

模型/这个函数其实是在计算下一个词的可能性并输出。

### 本质：一种文本的有损压缩形式

LLM从大量的文本数据提取出其中规律与关联（“有损压缩”），这些模式将以概率的形式体现在模型中：模型学习特定的字后跟另一个字的概率有多高，以此类推。

实现过程大致是：

- 生成随机值的参数——生成包含随机值和参数的张量（多维矩阵）
- 通过学习文本数据修改参数——向这些张量输入大量的文本数据（达到TB级别！），使模型能够学习所有数据之间的关系并识别它们之间的模式，这些模式以概率的形式存储在我们随机初始化的张量中

如果用一个非常高级的定义来描述LLMs，那就是“将一种语言（如英语）的概率分布压缩到一系列矩阵中”。

上面讨论的随机初始化在很大程度上不适用于我们，因为它非常昂贵（我们谈论的是大型模型的数百万美元）。

接下来的内容将以“对模型进行微调”为重点——也就是说，采用一个预先训练的模型并向其提供少量数据（通常为几 MB），以使其行为与您心目中的任何下游任务保持一致。

例如，如果需要编码助手模型，则可以根据编码示例对模型进行微调，依此类推。

## 架构（transformer架构）（待施工）



## 如何训练一个LLM？/训练方法

基本上有三种训练LLMs方法：**预训练**、**微调**和**LoRA/Q-LoRA**。

接下来将逐一介绍。

## **Pre-training** 预训练

在我们让LLM去帮助解决具体的问题之前，我们需要先教它很多基础的知识，比如**语言知识、复杂多层次的世界知识**，为以后解决更复杂的问题**打好基础**。

这一步，LLM将从**大量的文本数据**中**学习到广泛的知识和模式。**

步骤如下：

1. **数据收集**：首先，需要收集大量文本数据数据集，通常数据量达到 TB 级。这些数据是训练模型以理解和生成文本的基础。
2. **选择模型**：根据特定的任务需求，选择或创建适当的模型架构。模型架构定义了数据如何在网络中流动和被处理。
3. **分词器训练**：训练一个分词器以正确处理数据。分词器负责将文本有效地编码和解码，这是理解和生成文本的关键步骤。
4. **数据预处理**：使用分词器的词汇表对收集到的数据集进行预处理，将原始文本转换为适合训练的格式。（这一步是必要的，因为计算机不能直接理解和处理自然语言文本，而只能处理数字。）包括以下几个步骤：
    1. **文本分割成令牌（分词，Tokenization）**：
        - 这个过程涉及将连续的文本字符串分割成更小的单元，称为“令牌”（tokens）。这些令牌通常是单词、数字或标点符号。
        - 例如，句子“我爱自然语言处理”可能会被分割成令牌“我”、“爱”、“自然语言”和“处理”。
    2. **将令牌转换为数字ID**：
        - 为了让计算机能处理这些令牌，每个令牌都被映射到一个唯一的数字ID。这个映射过程通常使用分词器内建的词汇表完成。
        - 例如，如果词汇表是`{"我": 1, "爱": 2, "自然语言": 3, "处理": 4}`，那么上面的令牌将被转换为序列[1, 2, 3, 4]。
    3. **添加特殊令牌和注意力掩码**：
        - **特殊令牌**：在某些情况下，可能需要在令牌序列中插入特殊令牌。这些特殊令牌有多种用途，例如表示序列的开始（如`[CLS]`）和结束（如`[SEP]`），或用于分隔不同的句子等。
        - **注意力掩码**：这是一种机制，用于在模型处理输入数据时指示哪些令牌是重要的，哪些应该被忽略。它通常用于控制模型在处理像填充令牌这样无关紧要的令牌时的行为。
5. **模型预训练**
    1. **目标**：在预训练期间，模型**学习预测句子中的下一个单词**或利用大量可用数据来**填充缺失的单词**。此过程涉及通过迭代训练过程优化模型的参数，该过程可最大限度地提高在给定上下文的情况下生成正确单词或单词序列的可能性。
    2. **方法：**为了实现我们的目标，预训练阶段通常采用**自监督学习技术**的变体。
        1. **掩码语言建模（MLM）**：
            
            步骤如下：
            
            - **数据准备**：文本数据中随机选取一定比例的令牌（如15%），并隐藏这些令牌，即将这些令牌替换为一个特殊的掩码令牌（如`[MASK]`）。
            - **模型训练**：模型的输入是部分被隐藏的文本序列，任务是预测被掩码的令牌。模型必须利用未被掩码的上下文令牌来准确预测缺失的令牌。
            - **参数优化**：通过迭代的训练过程，模型的参数根据预测的准确性进行优化，使用如一些特定的损失函数来衡量预测令牌与实际令牌之间的差异，并据此调整参数。
            
            通过以这种方式对大量数据进行训练，该模型逐渐对语言模式、语法和语义关系有了丰富的理解。
            
        2. **因果语言建模（CLM）**：
            
            然而，当今最常用的方法是因果语言建模。
            
            与MLM不同，因果语言建模侧重于在给定前一个上下文的情况下预测句子中的下一个单词。遵循如下步骤：
            
            - **数据准备**：数据不进行掩码处理，保留原始的文本序列。
            - **模型训练**：模型一次处理一个令牌，预测序列中下一个即将出现的令牌。这种模式使模型在每个时间步只依赖于先前的令牌，反映了自然语言的生成过程。
            - **参数优化**：模型通过预测每个令牌后的下一个令牌来训练，利用如一些特定的损失函数来衡量预测令牌与实际令牌之间的差异，并据此调整参数。
            
            因此，因果语言建模更加适合于生成任务，因为它按照时间顺序处理和生成令牌，模拟人类说话或写作的自然流程。
            

6. 预训练的结果与不足

这个初始预训练阶段旨在捕获一般语言知识，使模型成为熟练的语言编码器。但是，它缺乏关于特定任务或领域的具体知识。为了弥合这一差距，在预训练之后将进行随后的微调阶段。

## **Fine-tuning** 微调

**微调是把模型的整个“脑袋”稍微调整一下，让它更聪明或者更适合特定任务。**

在最初的预训练阶段，模型学习到通用的语言知识。之后，通过微调，可以将模型的功能专门化，并在更窄、更特定于任务的数据集上优化其性能。

微调过程涉及几个关键步骤。

- **收集特定于任务的数据集**：首先，收集特定于任务的数据集，该数据集由与所需任务相关的标记示例组成。例如，如果任务是 指令调优，则需要收集 指令-响应对 的数据集。微调的数据集通常比预训练的数据集要小得多。
- **初始化预训练模型**：使用 预训练阶段学习到的参数 初始化模型。这意味着微调阶段不是从头开始，而是基于已经学到的通用语言知识进行调整。
- **训练模型**：在特定于任务的数据集上训练模型，优化其参数以 最小化 特定于任务的损失，即模型预测结果与所需结果之间的差异。
- **基于梯度的优化算法**：使用诸如随机梯度下降（[stochastic gradient descent ，SGD](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)）或 [Adam](https://arxiv.org/abs/1412.6980)等基于梯度的优化算法来调整预训练模型的参数。通过在模型层中反向传播损失，计算梯度，使模型能够从错误中学习并相应地更新其参数。*（后面的部分有梯度的内容~）*

为了优化微调效果，可以采用其他技术，例如学习率调度（[learning rate scheduling](https://d2l.ai/chapter_optimization/lr-scheduler.html)）、正则化方法（[dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) or [weight decay](https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab)）或提前停止以防止过拟合（ [early stopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/) to prevent [overfitting](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)）。这些技术有助于优化模型的泛化，并防止模型过于紧密地记住训练数据集。

## **Low-Rank Adaptation (LoRA) 低秩适应**

微调的计算成本很高，需要数百GB的VRAM来训练数十亿个参数模型。为了解决这个具体问题，提出了一种新方法：**低秩适应**。

与使用 Adam 微调 OPT-175B 相比，LoRA 可以减少 10,000 倍的可训练参数数量和 3 倍以上的 GPU 内存需求。（Refer to the paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) and the HuggingFace [PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft) blog post. ）

对于普通消费者来说，将内存需求降低 3 倍仍然不可行。于是引入了一种新的 LoRA 训练方法：**量化低秩适应 （QLoRA）**。

它利用  [bitsandbytes](https://github.com/timdettmers/bitsandbytes) 库对语言模型进行动态和近乎无损的量化，并将其应用于 LoRA 训练过程。这大大降低了内存需求—— 能够在 2 个 NVIDIA RTX 3090 上训练多达 700 亿个参数的模型！相比之下，您通常需要超过 16 个 A100-80GB GPU 来微调该尺寸级别的模型，相关成本将是巨大的。

下面将重点介绍微调和 LoRA/QLoRA 方法。

## **Fine-tuning 微调**

如前所述，微调可能很昂贵，具体取决于您选择的模型大小。通常至少需要 6B/7B 参数。我们将介绍一些获取训练计算的选项。训练计算，这一块要写吗？不确定！！！！

## **Gathering a Dataset 收集数据集**

毫无疑问，数据集收集是微调过程中最重要的部分。质量和数量都很重要，但质量更重要。

首先，想想你希望微调后的模型能做什么。 写故事？角色扮演？为你写你的电子邮件？也许你想创建你的 AI 对象。

让我们假设你想训练一个类似 [Pygmalion](https://huggingface.co/PygmalionAI) 的聊天和角色扮演模型。

接下来，你需要收集一个对话数据集，特别是包含互联网角色扮演风格的对话内容。收集这些数据可能会有些挑战性，你必须自己弄清楚:D

## **Dataset structure 数据集结构**

你的数据集的要求：

- **数据多样性**：您不希望模型只执行一项非常具体的任务。在我们假设的用例中，我们正在训练一个聊天模型，但这并不意味着数据将只涉及一种特定类型的聊天/RP。您将需要使训练样本多样化，包括各种方案，以便模型可以学习如何为各种类型的输入生成输出。
- **数据集大小**：与 LoRA 或Soft Prompts相比，您需要相对大量的数据。当然，这与预训练数据集不在同一级别。根据经验，请确保至少有 10 MiB 的数据用于微调。过度训练模型非常困难，因此你可以堆叠更多数据。
- **数据集质量**：数据质量非常重要。您希望数据集能够反映模型的结果。如果你给它喂垃圾，它会吐出垃圾。

## **Processing the raw dataset 处理原始数据集**

你现在可能有一堆文本数据。在继续之前，您需要将它们解析为适合预处理的格式。

如果您从网站抓取数据，则可能有 HTML 文件。在这种情况下，您的首要任务是从 HTML 元素中提取数据。

如果您从在线开放数据源获取数据集，则可能拥有 CSV 文件。

或者您从SQL中获取数据集，可能会更难一些。

具体的实现过程，可以去看后面的项目。

## **Minimizing the noise 将噪音降至最低**

**最好的语言模型是随机的。**这使得我们很难预测它们会如何生成（即使输入提示保持不变）。这有时会导致低质量和不良输出。

**您需要确保清除数据集中不需要的元素。**

如果您的数据源是合成的，即由 GPT-4/3 生成，这一点就显得尤为重要。

您可能希望过滤删除提及的一些短语，例如“作为 AI 语言模型...”、“有害或令人反感的内容......”、“......由 OpenAI 训练......“等。[ehartford](https://huggingface.co/ehartford) 的[这个脚本](https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered/blob/main/optional_clean.py)是此特定任务的良好过滤器。你也可以参考 https://github.com/AlpinDale/gptslop 仓库。

## **Starting the training run开始训练运行**

我们将使用[OpenAccess](https://github.com/OpenAccess-AI-Collective/axolotl)进行微调，因为它使用简单且具有我们需要的所有功能。

如果您使用的是 RunPod 等云计算服务，您可能具备所有必要的要求。

1. Clone the repository and install requirements:
    
    克隆存储库和安装要求：
    
    ```bash
    git clone https://github.com/OpenAccess-AI-Collective/axolotl && cd axolotl
    pip3 install packaging
    pip3 install -e '.[flash-attn,deepspeed]'
    ```
    

这将安装axolotl，然后我们就可以开始微调了。

axolotl将所有训练选项都放在一个 `yaml` 文件中。 `examples` 目录中已经有一些示例配置，适用于各种不同的模型。

在此示例中，我们将使用 QLoRA 方法训练 Mistral 模型，这应该可以在单个 3090 GPU 上实现。要开始运行，只需执行以下命令：

```bash
**accelerate launch -m axolotl.cli.train examples/mistral/config.yml**
```

恭喜！你刚刚训练了Mistral！示例配置使用一个非常小的数据集，应该需要很少的时间来训练。

若要使用自定义数据集，需要将其正确格式化为 `JSONL` 文件。

axolotl有许多不同的格式，你可以在[这里](https://github.com/OpenAccess-AI-Collective/axolotl#dataset)找到例子。

然后， `qlora.yml` 您可以编辑文件并将其指向您的数据集。

所有配置选项的完整说明都在[这里](https://github.com/OpenAccess-AI-Collective/axolotl#dataset)，请确保单击展开按钮以查看所有选项！

您现在知道如何训练模型，但让我们在下一节中介绍一些非常重要的信息。我们将首先解释 LoRA 到底是什么，以及为什么它有效。

---

## **Low-Rank Adaptation (LoRA) 低秩适应**

LoRA就像是在预训练好的模型上**加上一层专门的“滤镜”**，让它能在特定任务上表现得更好，就像在原有的画作上加一层滤镜，而不破坏原画。

**“原有的画作”**：在预训练阶段，模型通过大量的数据学习到了一般性的知识和模式，这些知识以权重的形式存储在模型的神经网络中。

**“滤镜”**：LoRA在现有权重中引入成对的秩分解权重矩阵（称为更新矩阵）。LoRA只专注于训练这些新添加的权重

LoRA能够加快大型语言模型的训练过程，同时减少内存消耗。它具有以下几个优点：

1. **保留了预训练模型中的原始权重**：应用LoRA的过程中，模型的原始能力和知识不会被改变或丢失。这将灾难性遗忘的风险降至最低，确保了模型在适应新数据的同时保留其现有知识。
2. **训练权重的可移植性**：与原始模型相比，LoRA 中使用的秩分解矩阵的参数要少得多。这一特性使得经过训练的 LoRA 权重能够轻松地在不同环境中转移和使用，非常便携。
3. **与原始模型的注意力层集成**：LoRA 矩阵通常合并到原始模型的注意力层中。此外，这里提供了可以用来调节的 适应量表参数 ，它可以控制模型适应新训练数据的程度，即控制模型在面对新数据时，变化的幅度有多大。
4. **提高内存效率**：LoRA 提高了内存效率，只需要不到原来微调所需计算量的三分之一，就能完成微调任务。这使得模型微调更高效、更经济。

## **LoRA hyperparameters LoRA超参数**

在介绍各个超参数之前，我们先来了解一下模型参数与超参数的区别。

<aside>
💡 **模型参数：**

1. **定义**：模型参数是在训练过程中通过数据学习到的参数，是模型用来进行预测的实际参数。
2. **设置方式**：模型参数通过训练算法（如反向传播）从数据中自动学习和更新。
3. **作用**：模型参数直接决定了模型的预测能力，是模型在看到新的输入时生成输出的关键。
</aside>

<aside>
💡 **超参数**

1. **定义**：超参数是在训练之前设定的参数，用于控制训练过程和模型架构。
2. **设置方式**：超参数由研究者或工程师手动设定，通常需要通过实验或调参方法（如网格搜索、随机搜索）来确定最佳值。
3. **作用**：超参数影响模型训练的效率和最终性能，但它们不是通过训练过程自动学习到的。
</aside>

### LoRA Rank 秩

**这决定了秩分解矩阵的维度大小。**

Rank越高，结果越好，计算要求也越高。

数据集越复杂，rank就越高。

最初的 LoRA 论文（[LoRA paper](https://arxiv.org/pdf/2106.09685.pdf) ）建议将 8 （ `r = 8` ） 作为最小值。

要匹配完全微调，您可以将rank设置为等于模型的隐藏大小。但是，不建议这样做，因为这会浪费大量资源。

您可以通过阅读 `config.json` 或使用 [Transformers](https://github.com/huggingface/transformers) 加载模型 `AutoModel` 并使用以下 `model.config.hidden_size` 函数来找出模型的隐藏大小：

```python
from transformers import AutoModelForCausalLM
model_name = "huggyllama/llama-7b"      # can also be a local directory
model = AutoModelForCausalLM.from_pretrained(model_name)
hidden_size = model.config.hidden_size
print(hidden_size)
```

### LoRA Alpha 超参数

它决定了模型在微调时，LoRA 矩阵对模型权重**调整的幅度**。通过调整 Alpha，可以控制 LoRA 在新任务中的适应能力。

在 LoRA 中，低秩矩阵通常与模型的原始权重矩阵相加。

Alpha 用于**缩放**这个低秩矩阵，使得微调过程对原始权重的影响可以灵活调节。

**小 Alpha 值**：如果 Alpha 值较小，LoRA 对模型权重的调整较少，模型对新数据的适应性较低，但保持了对原始任务的良好性能。
**大 Alpha 值**：如果 Alpha 值较大，LoRA 对模型权重的调整较多，模型对新数据的适应性较高，但可能会在原始任务上表现稍差。

## LoRA **Target Modules 目标模块**

> 在一个复杂的模型中，有很多不同的模块，每个模块都有它自己的参数。我们可以选择其中的一部分作为我们的**目标模块**，专注于改进和优化这些模块。这样做可以使模型在特定的任务上表现得更好，同时也可以提高训练的效率。
> 

### 如何确定目标模块

我们可以选择模型中某些特定的权重和矩阵进行训练。

<aside>
💡 在所有的权重和矩阵中，最基本和常用的训练对象是查询向量（Query Vectors）和值向量（Value Vectors）的投影矩阵。

查询向量的投影矩阵通常叫做 `q_proj`。

值向量的投影矩阵通常叫做 `v_proj`。

</aside>

但是，不同的模型可能会使用不同的名称来表示这些矩阵。

您可以通过运行以下脚本来查找确切的名称：

```python
from transformers import AutoModelForCausalLM
model_name = "huggyllama/llama-7b"      # 这个名字也可以是一个本地的目录
model = AutoModelForCausalLM.from_pretrained(model_name)
layer_names = model.state_dict().keys()

for name in layer_names:
    print(name)
```

这个程序会输出一长串名字，就像这样：

```python
model.embed_tokens.weight
model.layers.0.self_attn.q_proj.weight
model.layers.0.self_attn.k_proj.weight
model.layers.0.self_attn.v_proj.weight
model.layers.0.self_attn.o_proj.weight
...
model.norm.weight
lm_head.weight
```

接下来是对每个模块的基本说明。

### 模块的基本说明

命名约定基本上是： `{identifier}.{layer}.{layer_number}.{component}.{module}.{parameter}` .

注意！下面这些名称对于每个模型架构都不同

1. `up_proj`（向上投影矩阵）

- **功能**：用于从解码器传递到编码器的注意力机制。
- **作用**：将解码器的隐藏状态转换为与编码器隐藏状态相同的维度，以便在注意力计算期间保持兼容性。
- **举例**：就像我们把一个信号从一个设备发送到另一个设备时，需要确保信号格式兼容，`up_proj` 就是用来做这种转换的。

2. `down_proj`（向下投影矩阵）

- **功能**：用于从编码器传递到解码器的注意力机制。
- **作用**：将编码器的隐藏状态转换为解码器预期的维度，以便进行注意力计算。
- **举例**：就像我们把一种语言翻译成另一种语言，确保解码器可以理解来自编码器的信息。

3. `q_proj`（查询投影矩阵）

- **功能**：应用于注意力机制中的查询向量。
- **作用**：将输入的隐藏状态转换为所需的维度，以实现有效的查询表示形式。
- **举例**：就像在搜索引擎中，我们输入查询词，然后系统将其转换为能在数据库中有效查找的格式。

4. `v_proj`（值投影矩阵）

- **功能**：应用于注意力机制中的值向量。
- **作用**：将输入的隐藏状态转换为有效值表示的所需维度。
- **举例**：就像我们找到了想要的信息，并将其转换成我们可以理解和使用的格式。

5. `k_proj`（关键投影矩阵）

- **功能**：应用于注意力机制中的关键向量。
- **作用**：帮助模型确定哪些信息是重要的（关键的），并将这些信息转换为合适的维度。
- **举例**：就像你在准备考试，需要复习很多书，但是时间有限。`k_proj`就像一个老师，他帮你挑选出考试最重要的部分，让你重点复习这些内容。

6. `o_proj`（输出投影矩阵）

- **功能**：应用于注意力机制的输出。
- **作用**：将组合后的注意力输出转换为所需的维度，以便进一步处理。
- **举例**：就像在一个团队会议中，汇总所有人的意见后，整理成一个清晰的报告进行下一步的决策。

但是，有三个（或 4 个，如果您的模型有偏差）异常值。它们不遵循上面指定的命名约定，省略了图层名称和编号。这些是：

1. **Embedding Token Weights** 

- 人话版：这个层是模型的**起点**，它把输入的单词或者标记转换成计算机能理解的形式。这些转换后的形式叫做**密集向量**。
- `embed_tokens`表示与模型嵌入层关联的参数，通常放置在模型的开头，因为它用于将输入标记或单词映射到其相应的密集向量表示。
- 如果数据集具有自定义语法，则需要注意它能否准确“翻译”。

**2. Normalization Weights `norm`**

- 模型中的归一化层。
- 人话版：当我们在训练神经网络时，数据会在各个层之间传递。在每一层，数据会经过各种计算，比如加法、乘法等。`norm`会调整数据，让它们在一个合适的范围内，这样模型能更好地学习。
- 层或批量归一化通常用于提高深度神经网络的稳定性和收敛性。它们通常放置在模型架构的某些层内或之后，以缓解梯度消失或爆炸等问题，并帮助加快训练速度和更好的泛化。
- 通常不针对 LoRA。

**3. Output Layer `lm_head`**

- 语言模型的输出层。
- 人话版：这个层是模型的最后一步，它根据前面的层学到的信息来预测下一个单词。
- 它负责根据从前几层学习到的表示形式为下一个令牌生成预测或分数，放在底部。
- 如果数据集具有自定义语法，则需要注意是否符合该语法。

---

## **QLoRA** 量化低秩适应

我们首先要了解一下量化的概念。

💡 量化：我们会发现，高清的图片往往比模糊的图片占有更多的空间。有时即使图片有些模糊，我们也可以看清主体。量化就像是把图片的像素变低，使它变得模糊，但仍保留主要特征。

在计算机里，量化是把复杂的数值数据用更简单的数值来表示。比如：

- **浮点数量化到整数**：原来的数据是非常精确的浮点数（小数），现在你把它们变成了整数。虽然精确度降低了，但计算起来更简单，更快。
- **低位量化**：原来每个数值可能需要很多位（二进制位）来表示，现在你用更少的位来表示。比如说，从32位变成8位，这样数据的体积就变小了，占用的存储空间也减少了。

**量化的好处：**

- **减少存储空间**：数据变小了，占用的存储空间也就少了。
- **加快计算速度**：简单的数据处理起来更快，计算效率提高了。
- **节省资源**：更少的存储空间和计算资源意味着更低的成本。

QLoRA，它不仅使用了LoRA的方法，还通过量化来进一步减小模型的大小和复杂度。

由于QLoRA多了量化这一步，所以相比LoRA，QLoRA生成的模型会更小、更紧凑，也实现过程更复杂一些。

由此，QLoRA 是一种高效的微调方法，可减少内存使用量，同时保持大型语言模型的高性能。

它支持在单个 48GB GPU 上微调 65B 参数模型，同时保留完整的 16 位微调任务性能。

QLoRA 的主要创新点包括：

- 在训练过程中，预训练的语言模型参数**被量化为 4 位，并被冻结**，不参与训练过程。只有低秩适配器（LoRA）的参数会在反向传播过程中更新，这样大大减少了计算量和存储需求。
- 使用 **4 位 NormalFloat（NF4）**（一种专门设计的数据类型），能够以最佳方式处理正态分布的权重。这种数据类型有助于在保持模型精度的同时减少计算和存储资源的消耗。
- **双重量化技术**：不仅对模型权重进行量化，还对这些量化权重的量化常数本身也进行量化，从而大大减少了所需的存储空间。
- **分页优化器：**用于在微调过程中有效管理内存峰值。就像分批搬运重物一样，分页优化器通过分阶段处理来减轻内存的负担。
    
    

在接下来的章节中，我们将了解所有训练超参数（也称为配置）的作用。

## **Training Hyperparameters** 训练超参数（待施工）

**训练超参数**是指导训练过程的设置，用于确定模型如何从提供的数据中学习。这类超参数的选择会显著影响训练的效果（模型的收敛性、泛化性和整体有效性）

在本节中，我们将解释在训练阶段几个关键的训练超参数，带你深入了解这些训练超参数的概念以及影响，帮助你更有效地优化模型。

为了更好地理解它们的定义与影响，我们以梯度下降算法作为背景来说明，通过理解它们在梯度下降中的作用可以帮助你更好地调节这些参数。

首先，我们来看看梯度下降

## **Batch Size and Epoch 批量大小和纪元**

## **Interpreting the Learning Curves 解释学习曲线**

如果我们想直观的了解我们的学习情况变化，我们可以通过一种方式来进行评分（比如在高中会关注正答率），并记录连续一段时间评分，将它画成图表。这样，就可以直观地看到学习评分随着时间的变化。

同理，我们也可以通过学习曲线，了解模型在训练过程中的性能变化。

## 如何“评分”？
