# 核心概念/知识
本部分包括了大多数学习本课程时必要的关键概念和知识,其中非常重要的内容我们会通过==这种高亮==表示.由于笔者水平有限,这个部分我们主要推荐一些资源.由于各位学习者的基础不同,我们建议学习者自己根据知识掌握情况灵活选择资源来学习.
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px; margin-bottom: 10px">
<p>我们会在类似这样的框中给出一些问题.</p>
<p>你不一定要回答所有这些问题,但是起码大部分问题你都应该可以回答上.</p>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
        <details>
            <summary style="margin-bottom: 3px">Answer</summary>
            就是类似于这样的折叠部分会隐藏解答
        </details>
    </div>
</div>

## 数学
### ==神经网络==
神经网络和机器学习是整个课程核心中的核心,虽然在课程内容设置时不会假设学习者相当了解有关概念,但是了解相关基础知识还是很有用的,起码不会让前期学习曲线过于陡峭.这里我们推荐的速通教程是3B1B的这几个神经网络相关视频.
- [【官方双语】那么什么是神经网络? | 深度学习Part1](https://www.bilibili.com/video/BV1bx411M7Zx)
- [【官方双语】梯度下降,神经网络是怎么学习的 | 深度学习Part2](https://www.bilibili.com/video/BV1Ux411j7ri)
- [【官方双语】反向传播算法究竟做了什么? 上/下 | 深度学习Part3](https://www.bilibili.com/video/BV16x411V7Qg)
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px; margin-bottom: 10px">
<strong>是什么让神经网络比线性回归等基本统计方法有更强大的表达能力?</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
        <details>
            <summary style="margin-bottom: 3px">Answer</summary>
            <p>一些可能的需要考虑的要点:</p>
            <li>神经网络利用了他<strong>非线性</strong>的特点,这使得他可以表达更多的函数,更有可能接近真实分布.而线性回归则相对有限.</li>
            <li>神经网络利用梯度下降来学习,这意味着他的表达能力不会受到程序员或数学家手动设计的拟合算法的限制.</li>
        </details>
    </div>
</div>
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px; margin-bottom: 10px">
<strong>比起sigmoids, ReLU作为激活函数有什么优势?</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
        <details>
            <summary style="margin-bottom: 3px">Answer</summary>
            <p>一些可能的需要考虑的要点:</p>
            <li>ReLU更有效的避免了梯度消失的问题.如果使用sigmoid这是十分常见的问题.</li>
            <li>ReLU的计算效率比sigmoid更高</li>
            <p>然而,对于ReLU甚至大部分机器学习的理论来说最重要的一点是--更好的实验效果往往比理论验证更先出现! 许多机器学习的理论都建立在这样的理念上:"不断的实验直到找到有效的方法,<em>随后</em>再搞明白为什么更有效."</p>
        </details>
    </div>
</div>

### ==线性代数==

![](/3.AI模块/static/linear_algebra.png)
线性代数是整个机器学习的核心中的核心内容.下面是一些你需要非常熟悉的内容:
- 线性变换(linear transformations)-什么是线性变换以及它为什么很重要
- 矩阵乘法工作的原理
- 矩阵的基本属性: 秩(rank), 迹(trace, 当然国内大部分情况下叫它对角线), 行列式(determinant), 转置(transpose)
- 基(base)和基本的变换(transformations)
这里还有一些非必要的内容.他们也很重要,但是你不需要非常熟练掌握:
- [奇异值分解(Singular value decomposition, SVD)](https://www.lesswrong.com/posts/iupCxk3ddiJBAJkts/six-and-a-half-intuitions-for-svd)
- 特征值(eigenvalues)和特征向量(eigenvectors)
- 不同类型的矩阵及其意义(例如对称(symmetric)矩阵,正交(orthogonal)矩阵,单位(identity)矩阵,旋转(rotation)矩阵)
同样的,这里我们最推荐的速通学习资源仍然是[3B1B的视频系列](https://www.bilibili.com/video/BV1rs411k7ru).如果你有更多的时间,也可以考虑学习[Linear Algebra Done Right](https://linear.axler.net/).此外,Neel Nanda有两个[Youtube视频](https://www.youtube.com/watch?v=0EB23unfLSU)广泛的介绍了线性代数.
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px; margin-bottom: 10px">
    <strong>如果在建立神经网络的时候仅仅使用线性变换会出现什么问题?</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
    <details>
        <summary style="margin-bottom: 3px">Answer</summary>
        <p>线性变换的组合也是能等效成线性变换的,如果仅仅使用线性变换只能得到线性函数,然而在大部分情况下我们想学到的函数都是非线性的.</p>
        <p>但是如果将线性变换和非线性变换组合我们基本上可以学到任何东西!</p>
    </details>
    </div>
</div>
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px;margin-bottom: 10px">
    <strong>矩阵
    <span class="katex">A</span>
    和
    <span class="katex">B</span>
    的形状分别为
    <span class="katex">(n,m)</span>
    和
    <span class="katex">(m,l)</span>
    .矩阵
    <span class="katex">AB</span>
    最大可能的秩是多少?</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
    <details>
        <summary style="margin-bottom: 3px">Answer</summary>
        <p>AB的秩不大于A和B的秩的最小值决定,且矩阵的秩不大于其行数和列数的最小值,所以rank(AB)<=(n,m,l)</p>
        <p>注意: 在transformer中,经常会遇到m << n,l的情况,这意味着我们经常会遇到规模很大但是低秩的矩阵.对于这种情况我们会使用奇异值分解等工具来分析矩阵(例子见<a herf=https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight>这篇文章</a>).</p>
    </details>
    </div>
</div>

### ==概率论与数理统计==
了解什么是概率,什么是期望,什么是标准差并理解他们是很重要的,同时了解独立性和高斯分布对于学习本课程也是很有益的.
这个[链接](https://medium.com/jun94-devpblog/prob-stats-3-expected-value-variance-and-standard-deviation-bce9303d8da8)涵盖了以上的一些要点
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px;margin-bottom: 10px">
    <strong>任意两个独立的服从高斯分布的随机变量之和的期望和方差是多少?如果这两个随机变量相关的话结果会有什么不同?(后一问仅需要定性回答)</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
        <details>
            <summary style="margin-bottom: 3px">Answer</summary>
            <p>对于独立随机变量,其期望为两者期望之和,其方差为二者方差之和</p>
            <p>如果两个变量相关,那他们的期望仍然是二者期望之和,但是方差结果会有所不同: 如果二者正相关方差会更大,二者负相关方差会更小.有一种经验判断方法是如果两个变量完全相关,那二者和基本就是单个值翻倍,那么他们的方差会变为原来的4倍;如果两个变量完全反相关,那二者和始终为0,因此方差为0.</p>
        </details>
    </div>
</div>

### ==微积分==
理解微积分也是很重要的,同时了解向量微积分包括链式法则和泰勒级数的基础知识对于学习本课程也是很有益的.
同样,我们还是推荐3B1B关于这方面的视频: [微积分的本质](https://www.bilibili.com/video/BV1cx411m78R)
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px;margin-bottom: 10px">
    <span class="math math-inline" data-immersive-translate-walked="385c02cf-0d5e-467c-aaab-1943f1db03dd"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><mi>y</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(x, y) = \frac{1}{2}(x-y)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><strong>关于自变量</strong><span class="math math-inline" data-immersive-translate-walked="385c02cf-0d5e-467c-aaab-1943f1db03dd"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span><strong>的微分是什么(这里认为所有的变量都是标量,不考虑向量)?对于交叉熵损失的式子</strong><span class="math math-inline" data-immersive-translate-walked="385c02cf-0d5e-467c-aaab-1943f1db03dd"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo stretchy="false">(</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mi>x</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(x, y) = -(y\log{x} + (1-y)\log{(1-x)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">−</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop">lo<span style="margin-right: 0.01389em;">g</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop">lo<span style="margin-right: 0.01389em;">g</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span><span class="mclose">)</span></span></span></span></span><strong>结果是什么?考虑</strong><span class="math math-inline" data-immersive-translate-walked="385c02cf-0d5e-467c-aaab-1943f1db03dd"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x \in (0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span><strong>且y是取值为0或1的分类标签.定性描述一下对于这些损失函数在执行梯度下降操作时x的行为.</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
        <details>
            <summary style="margin-bottom: 3px">Answer</summary>
            <p><strong>MSE loss</strong></p>
            <p>对于x微分结果为x-y.定性来看,这意味着当x < y时x将会增大,且当x > y时x将会减小.减小和增大的大小与二者间的差值成正比.</p>
            <p><strong>BCE loss</strong></p>
            <p>对于x微分结果为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mi>y</mi><mi>x</mi></mfrac><mo>+</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>y</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\frac{y}{x} + \frac{1-y}{1-x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.0925em; vertical-align: -0.345em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7475em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.4461em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.3005em; vertical-align: -0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8972em;"><span style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span><span style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span style="top: -3.4461em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right: 0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>.因为y仅有0和1两个取值,所以可以分开讨论.</p>
            <p>定性来说,当y=1时x在梯度下降时将会增大,当y=0时x在梯度下降时会减小;且随着x越接近1-y(即远离正确值的情况)会让x增大/减小的幅度变得无限大.</p>
            <p>注意--在这种情况下,x是模型输出的概率.这可能是模型直接输出的logit值经过sigmoid或softmax后的结果.所以使用BCE损失函数并不意味着模型权重的梯度是无限的.</p>
        </details>
    </div>
</div>

### 信息论

理解什么是信息,熵,KL散度是很有益的,这在解释损失函数的时候会很有用.
Thomas M. Cover的[Elements of Information Theory](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf)是Jacob Hilton推荐的教科书.它涵盖的内容可能超过了你需要了解的内容,所以你也不必让它在你的待学列表上优先级特别高.
关于KL散度(信息论和机器学习中一个很重要的概念)有关的内容可以参考[Six (and a half) intuitions for KL divergence](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence).请注意,如果你还没充分掌握什么是熵,直接了解KL散度有关内容可能没什么意义.
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px;margin-bottom: 10px">
<strong>设P是自然语言中一句话的下一个词的分布概率,Q是语言模型估计的同一句话下一个词的概率分布.如果模型以同样的概率预测每一个词,的那交叉熵H(P,Q)是什么?如果模型猜测的下一词概率完全准确,交叉熵又是什么?</strong>
    <div style="background-color: #FFFFFF; padding: 10px; border-radius: 5px; margin-bottom: 10px">
        <details>
        <summary>Answer</summary>
        <p>如果模型是平均猜测的,那交叉熵为log|V|,其中V代表词库,|V|代表词库中所有词的总数.</p>
        <p>如果模型猜测完全准确,那交叉熵为H(P).换句话说,交叉熵退化为了自然语言下的熵.</p>
        </details>
    </div>
</div>

## 编程
### ==Python==
精通Python是非常非常重要的,因为本课程需要完成的所有代码都是Python实现的.作为粗略的指示,我们希望你至少能熟悉[这里](https://book.pythontips.com/en/latest/)至少80%-90%的内容,直到"**21.for/else**"部分.要更彻底的了解Python的核心功能,可以参考[这里](https://docs.python.org/3/tutorial/)

### 库
#### ==[NumPy](https://numpy.org/)==
熟悉NumPy是高效的使用Python的必备条件.此外,NumPy数组的大部分操作和PyTorch的张量语法非常相似,通常只有细微的差别(例如,同样表示维度,NumPy中常用axis关键字,而PyTorch中常用dim).完成这里的[100个NumPy练习](https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises.ipynb)是个学习NumPy的好方法.或者你认为你已经很熟悉NumPy,那么可以接着往下看.

#### ==[PyTorch](https://pytorch.org/)==
从第0章开始,我们会进行一些结构化的练习,旨在让每位学习者熟悉PyTorch.随着你使用PyTorch越来越舒服,你可能会发现你越来越熟悉PyTorch.PyTorch有许多有用的教程,为了能更轻松的使用张量,你可以考虑使用PyTorch实现上面链接的100个Numpy练习.还有一种选择是学习[基础知识教程](https://pytorch.org/tutorials/beginner/basics/intro.html),你也可以跳过这一步如果你能清晰的解释下面的几个问题.
<div style="background-color: #FFDDDD; padding: 10px; border-radius: 5px;margin-bottom: 10px">
    <li>从高层次来看,什么是<code>torch.Tensor</code>?</li>
    <li><code>nn.Parameter</code>和<code>nn.Module</code>是什么?</li>
    <li>当你调用<code>.backward()</code>时,你的梯度都存储在哪里?</li>
    <li>什么是损失函数?一般来说,他需要什么参数,又会返回什么内容?</li>
    <li>优化算法做了什么事?</li>
    <li>什么是超参数?他和一般的参数有什么不同?</li>
    <li>有什么超参数的例子?</li>
</div>

#### ==[Einops](https://einops.rocks/1-einops-basics)和[Einsum](https://einops.rocks/api/einsum/)==
这些都是用来进行张量操作的库.如果你可以熟练掌握,就可以摆脱笨拙的Numpy/PyTorch方法(例如转置transpose,排列permute,挤压squeeze等等).课程后续有一些einops和einsum的练习,随着你对于这两个库越来越熟悉,完成的速度会越快.
对于einops的熟悉,你可以通读示例直到"Fancy examples in random order"部分.你可以在自己的Jupyter Notebook里面尝试这些操作来熟悉他们.
对于einsum,[这个页面](https://rockt.github.io/2018/04/30/einsum)提供了基本介绍,并展示了一些示例.请注意,我们将要使用的是einops库中的einsum函数,他允许你在指代维度时可以使用名称而不是单个字母.

#### ==[Typing](https://docs.python.org/3/library/typing.html)==
对Python函数进行类型检查是找bug并保持代码清晰易读的好方法.众所周知,Python不是强类型的语言,这意味着你通常不会因为使用不正确数据类型导致报错,除非你在用MyPy这样的库.但是如果你用VSCode的话,你可以结合这个库实现真正有用的自动类型检查.

#### [Plotly](https://plotly.com/python)
Plotly是一个交互式的图形库,非常适合呈现结果和数据.如果你已经非常熟悉其他的Python绘图库例如matplotlib的话,那么我不建议重新学习Plotly.但是如果你还不太熟悉matplotlib或者你愿意学习Plotly的话,我还是强烈建议你尝试一下.

#### [Streamlit](https://share.streamlit.io/)
Streamlit是一个很酷的库,用于构建和分享基于数据的应用.他和Plotly集成得非常好,可以托管在你个人Github上,并相较于其他具有类似功能的库(例如Dash)来说他更加直观且易于学习.这不是强制学习的内容,但是如果你特别喜欢streamlit的外观的话你可以考虑用他来完成一些项目.请参阅[这里](https://copy-suppression.streamlit.app/)作为Streamlit可以做什么的范例.

## 软件工程
### ==基本编程技能==
如果你来学习这门课程,这个部分的能力你应该没有一点问题了.话说回来,多精进一下这方面的能力也没有什么坏处.LeetCode是一个保持基本编程技能不退化的好地方,特别是LeetCode上的难度分级和标准实现会对你的学习有所帮助.[Project Euler](https://projecteuler.net/)上的练习题也是可以看看的.

### ==VSCode==
虽然可能你们已经能熟练使用Jupyter Notebook了,但我还是建议使用VSCode来进行结构化的练习.这是一个功能强大的文本编辑器,提供了比Jupyter Notebook更多的功能,例如:
- 快捷键
这比Jupyter Notebook提供的任何功能都要强大.这里是一张特别有用的快捷键表格,更多的信息可以看[这个链接](https://www.geeksforgeeks.org/visual-studio-code-shortcuts-for-windows-and-mac/).

|  命令   | Windows/Linux  | Mac|
|  :----: | :----: | :----:|
|删除一行|Ctrl + Shift + K|Cmd + Shift + K|
|向上或向下复制一行|Shift + Alt + Up arrow 或 Shift + Alt + Down arrow|Opt + Shift + Up arrow 或 Opt + shift + Down arrow|
|全局查找|Ctrl + Shift + F|Cmd + Shift + F|
|Copilot|Ctrl + i / Ctrl + Shift + i| Cmd + i / Cmd + Shift + i|
|块注释|Ctrl + Shift + /|Cmd + Shift + /|
|命令面板|Ctrl + Shift + P|Cmd + Shift + P|
|行注释|Ctrl + /|Cmd + /|
|触发IntelliSense|Ctrl + Space|Cmd + Space|
|隐藏/打开侧边栏|Ctrl + B| Cmd + B|
|光标多选|Ctrl + D|Cmd + D|
|快速打开|Ctrl + P|Cmd + P|
- 类型检查
我们在上一小节讨论了Typing库.如果把这个库和VSCode的类型检查器图拓展一起使用会实现非常强大的功能.如果要打开这个功能,你可以在VSCode里进入`settings.json`文件,并添加以下行来激活:
```json
{
    "python.analysis.typeCheckingMode": "basic"
}
```
你可以先打开VSCode的命令面板(参考上面的快捷键),然后找到`首选项:打开用户设置(JSON)`来打开`settings.json`文件.我们不会在课程里使用这样的类型检查,因为有时候他可能会太严格,但是了解他还是很有用的.
- 笔记本功能
虽然VSCode里面提供了类似Jupyter Notebook的拓展,但他还有一个更有用的功能.通过在Python文件里添加充当单元格分隔符的`#%%`行,可以让Python文件变得像笔记本一样.通过这种方式,你可以分离代码块并单独运行他们.参阅[此页面](https://code.visualstudio.com/docs/python/jupyter-support-py)以获取进一步说明.
- Debugger
VSCode的Debugger是一个很好用的工具,比添加行来打印输出有关信息的标准方法更强大更高效! 你可以在代码中设置断点,并仔细检查程序中该点存储的局部变量.参阅[此页面](https://lightrun.com/debugging/debug-python-in-vscode/)以获取进一步说明.
- 测试
VSCode提供了轻松追中和运行使用pytest或unittest库编写的测试的方法.我们的课程中将涉及前者.
- 远程连接
VSCode提供了一个很简单的途径让你能通过SSH在远程服务器上运行代码.
- Copilot
Github Copilot使用OpenAI Codex来直接从整个编辑器实时建议代码和函数.我们在课程中鼓励(尽管不强制)使用Copilot.他不能做到所有事情,但是他非常有助于将你从烦人的低级细节中抽象出来,使你能更专注于更高级的概念和结构.
- Local imports
如果你在本地import的时候在句子下面看到了黄色的曲线(或者improt的库不起作用),请将其添加到工作区的JSON文件中(你可以在命令面板中搜索"workspace JSON"来访问他):
```json
{
    "python.analysis.extraPaths": [
        "extrapath"
    ],
    "python.analysis.include": [
        "extrapath"
    ]
}
```
其中`extrapath`是你要添加的路径(例如: `./chapter0_fundamentals/exercises`,如果你正在运行像这样的import语句`import part1_raytracing.solutions as solutions`).

### ==Jupyter Notebook / Colab==
尽管VSCode整体上非常棒,但Jupyter Notebook仍然比VSCode有一些优势,主要在探索代码和可视化结果方面.上一节列出的几个优点也适用于在VSCode中创建和编辑的`.ipynb`文件;我们不鼓励大家在面对ARENA项目的时候这样做的主要原因是我们希望你能在写新的代码时import前几天的代码(并且因为使用`.py`文件可以帮助更好的进行编程实践,而不是仅仅创建大量的单元格并放在不同的标题下,最后忘记了哪些代码放在哪里!).
Colab具有和Jupyter Notebook类似的结构,但它提供了一些附加功能,最重要的是访问GPU的权限.很遗憾我们没有办法为每一位学习者提供计算资源,因此(假设你没有其他途径获取计算资源的前提下)Colab可能是你的最佳选择.

### ==Git==
Git是一款版本控制软件,旨在跟踪和管理一组文件中的更改.它的功能非常强大,但是使用起来也稍微有一些门槛.
如果你从这个[仓库](https://github.com/callummcdougall/ARENA_3.0)中学习本课程,你需要熟练的使用Git来pull和push文件.我们也很高兴看到参与者在学习期间为开源库做出自己的贡献(例如TransformerLens或nnsight),以及使用GitHub进行协作.这些都需要熟练的使用Git.
![](/3.AI模块/static/git.png)

当然如果你已经有了强大的SWE背景,那这个部分应该不值得你花太多时间.如果你没有经验的话,我们推荐这个[Learn Git Branching](https://learngitbranching.js.org/)教程,这是一组直观且可交互的Git练习.这里再推荐一些其他资源,可能有重复:
- [给初学者的Git和GitHub简介](https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners)
- [沉浸Git](https://gitimmersion.com/index.html)
- [Git备忘单](https://www.atlassian.com/git/tutorials/atlassian-git-cheatsheet)

理想情况下,你应该能轻松的做到以下内容:
- Clone一个仓库
- 创建一个新分支并在分支间切换
- 暂存和提交更改
- Push你的分支

### ==Conda==
虚拟环境是在同一台设备上跨多个不同项目工作时管理依赖的常用方法,并且在所有专业开发情景下是标准操作.在ARENA学习中,我们预计参与者将在一个或更多环境中工作.
如果你现在还没用使用conda,请确保你已经安装了conda并知道如何创建激活虚拟环境.请参阅[conda入门](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).

### Unix
这对于本课程并不重要,但是他是你成为一个优秀软件工程师必备的技能.这个[UCB的UNIX教程](https://people.ischool.berkeley.edu/~kevin/unix-tutorial/toc.html)提供了全面的介绍(直到并包括第4节应该足以满足你在本课程中可能要做到的大多数事情).[Surrey University的UNIX教程](https://users.cs.duke.edu/~alvy/courses/unixtut/)也是很好的资源--直到教程2部分就足够了.
你可以根据[这里](https://cs50.readthedocs.io/terminal/)的说明设置UNIX终端来试验上面教程中介绍的方法.

## 额外的阅读材料推荐
我们假设你没有阅读过这些材料,但是他们都是需要了解且相关的内容,并且能让你在本课程涉及到有关内容时能更深入的了解.
- [100 NumPy Exercises](https://github.com/rougier/numpy-100)
完成课前练习后,如果你还想做更多的练习的话,请尝试这些.其中有一些比较有挑战性的你可以尝试再使用PyTorch实现他们.
- [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)**by Jeremy Howard**
在这里,你将自己实现torch.nn和torch.optim的许多功能.这是对这些包中功能的很好的介绍.如果你现在不学习这一点,你可以后续在课程中学习,尽管可能不是那么深入.
- [NLP Demystified](https://www.nlpdemystified.org/)**by Nitin Punjabi**
给没有ML背景的人看的NLP教程.如果你以前从未接触过NLP,值得浏览一下对该领域有总体的了解.如果你从未从头开始构建过基本的前馈神经网络(MLP),"Neural Networks I"部分是很好的练习.
- [Visualisinf Representations: Deep Learning and Human Beings](https://colah.github.io/posts/2015-01-Visualizing-Representations/)**by hris Olah**
通过精美的图片建立深层网络正在做什么的直觉.
- [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)**by OpenAI**
介绍使用深度学习进行强化学习.ARENA假设你没有RL学习的经验,但是如果你已经有了一点了解那你的学习过程会更轻松并能解决课程中更难的问题.
- [Introduction to Reinforcement Learning](https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver) **with David Silver**
该视频课程相当古老(2015),最前沿的技术已经有了很大的进展,但是对于了解基础知识仍然很有用.比起这个我更推荐Spinning Up in Deep RL,除非你从视频讲座中学到的东西比阅读更多.
- [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) **by Petersen and Pedersen**
密集罗列了关于矩阵的事实和公式.这本书当然不是为了教会你一些概念,而是你在查找你需要的内容的时候的好去处.第六页的1-6,11-16,18-23公式都是值得记住的.
- [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) **by Chris Olah et al**
一篇关于为计算机视觉训练的神经网络的可解释性的有趣的文章.
- [Why Momentum Really Works](https://distill.pub/2017/momentum/) **by Gabriel Goh**
使用动量的梯度下降的变体十分常见,我们在课程中会教动量的基本知识.但是如果你想要更丰富深入的理解动量,那这是一篇值得多读的好文章.
- [The Matrix Calculus You Need for Deep Learning](https://explained.ai/matrix-calculus/) **by Terence Parr and Jeremy Howard**
带你了解从微积分入门到矩阵微积分.这对于课程第三天的反向传播材料很有帮助.
- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) **by Anthropic**
从最简单的玩具模型逐步展开来分析transformer.这是一本很厚的书,但是对于建立关于transformer的直觉很有帮助.这将构成机理解释章节的基石.
- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) **by Anthropic**
描述并分析了transformer学习的重要组成部分"induction heads".
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) **by Michael Nielsen**
